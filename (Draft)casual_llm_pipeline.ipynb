{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5acf4520",
   "metadata": {},
   "source": [
    "# Data Loading + Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ce378",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1bb42",
   "metadata": {},
   "source": [
    "Load in the untransformed data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c49147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# File paths for each split (update these paths to your local files if needed)\n",
    "data_files = {\n",
    "    # \"train\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/all_prompts_train.jsonl\",\n",
    "    # \"validation\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/validation_prompts.jsonl\",\n",
    "    \"test\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/d2p_prompts_test.jsonl\"\n",
    "}\n",
    "\n",
    "# Load the dataset from JSONL files\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "print(raw_datasets)  # Display dataset splits and sizes\n",
    "# Each dataset item has 'prompt' and 'completion' fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afc8cb",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe8054",
   "metadata": {},
   "source": [
    "1. Tokenization\n",
    "2. Data Conversion: Raw Data Set Strucutre -> HF Casual LLM Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c50396",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids_list,\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask_list,\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels_list,\n\u001b[1;32m     74\u001b[0m     }\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing to the entire dataset\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mraw_datasets\u001b[49m\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# After this, each split in tokenized_datasets has columns: input_ids, attention_mask, labels.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m tokenized_datasets\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# Determine the device for training (the model is already on GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch, 'mps') and torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Choose a small causal model from Hugging Face (for example, LLaMA-2 7B or OPT 125M)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"  # e.g., LLaMA-2 7B model [oai_citation:2‡huggingface.co](https://huggingface.co/meta-llama/Llama-2-7b-hf#:~:text=Llama%202%20is%20a%20collection,the%20index%20at%20the%20bottom)\n",
    "# model_name = \"facebook/opt-125m\"    # (Alternatively, Gemma 2B instruction-tuned [oai_citation:3‡huggingface.co](https://huggingface.co/google/gemma-2-2b-it#:~:text=Gemma%20is%20a%20family%20of,helping%20foster%20innovation%20for%20everyone))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# If the tokenizer has no pad token (common for LLMs), assign the EOS token as the pad token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.padding_side = 'left'  # for decoder-only models, left-padding is often used\n",
    "\n",
    "# Define max sequence lengths for prompt and completion\n",
    "max_input_length = 128    # maximum tokens for the prompt\n",
    "max_target_length = 7    # maximum tokens for the completion/response\n",
    "total_max_length = max_input_length + max_target_length  # e.g., 192 tokens total\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Convert the data to the correct form that can be processed by the casual llm when training\"\"\"\n",
    "    prompts = examples[\"prompt\"]\n",
    "    completions = examples[\"completion\"]\n",
    "\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for prompt, completion in zip(prompts, completions):\n",
    "        # 1. Tokenize prompt & completion (no padding at this stage)\n",
    "        prompt_ids = tokenizer.encode(\n",
    "            prompt, add_special_tokens=False, truncation=True, max_length=max_input_length\n",
    "        )\n",
    "        comp_ids = tokenizer.encode(\n",
    "            completion, add_special_tokens=False, truncation=True, max_length=max_target_length\n",
    "        )\n",
    "\n",
    "        # 2. Concatenate prompt and completion token IDs\n",
    "        full_ids = prompt_ids + comp_ids\n",
    "        # Truncate to total_max_length if needed\n",
    "        full_ids = full_ids[: total_max_length]\n",
    "\n",
    "        # 3. Create labels:\n",
    "        #    - For prompt tokens and any padding, label is -100 (to ignore in loss).\n",
    "        #    - For completion tokens, label is the token ID.\n",
    "        labels = [-100] * len(prompt_ids) + comp_ids\n",
    "        labels = labels[: total_max_length]\n",
    "\n",
    "        # 4. Prepare attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = [1] * len(full_ids)\n",
    "\n",
    "        # 5. Pad sequences up to total_max_length\n",
    "        # (We pad here manually for clarity; alternatively, could use tokenizer.pad)\n",
    "        pad_len = total_max_length - len(full_ids)\n",
    "        if pad_len > 0:\n",
    "            full_ids = full_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask = attention_mask + [0] * pad_len\n",
    "            labels = labels + [-100] * pad_len\n",
    "\n",
    "        # Collect the processed sequence\n",
    "        input_ids_list.append(full_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list,\n",
    "    }\n",
    "\n",
    "# Apply the preprocessing to the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=[\"prompt\", \"completion\"])\n",
    "# After this, each split in tokenized_datasets has columns: input_ids, attention_mask, labels.\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92b47a",
   "metadata": {},
   "source": [
    "## Compressed Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5bf975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 3600\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3600\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from rc_experiment.rc_experiment.data_loading import raw_2_llm_data\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/all_prompts_train.jsonl\",\n",
    "    \"validation\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/validation_prompts.jsonl\",\n",
    "    \"test\": \"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/d2p_prompts_test.jsonl\"\n",
    "}\n",
    "\n",
    "# Determine the device for training (the model is already on GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if hasattr(torch, 'mps') and torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Choose a small causal model from Hugging Face (for example, LLaMA-2 7B or OPT 125M)\n",
    "model_name = \"meta-llama/Llama-3.2-1B\" \n",
    "\n",
    "# Define max sequence lengths for prompt and completion\n",
    "max_input_length = 128    # maximum tokens for the prompt\n",
    "max_target_length = 7    # maximum tokens for the completion/response\n",
    "total_max_length = max_input_length + max_target_length\n",
    "\n",
    "\n",
    "# NOTE: My function\n",
    "tokenized_datasets, tokenizer, device = raw_2_llm_data(data_files, model_name, max_input_length, max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a2b84",
   "metadata": {},
   "source": [
    "# Build the Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import random\n",
    "\n",
    "class promptCompleDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for QA pairs (prompt+completion) prepared for causal LM training.\"\"\"\n",
    "    def __init__(self, hf_dataset):\n",
    "        # Load all data from the Hugging Face dataset into memory as torch tensors\n",
    "        data = hf_dataset[:]  # get all items as a dict of lists\n",
    "        self.input_ids = torch.tensor(data[\"input_ids\"], dtype=torch.long)\n",
    "        self.attention_mask = torch.tensor(data[\"attention_mask\"], dtype=torch.long)\n",
    "        self.labels = torch.tensor(data[\"labels\"], dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self.input_ids.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a dictionary of tensors for the given index\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Instantiate dataset objects for each split\n",
    "train_dataset = promptCompleDataset(tokenized_datasets[\"train\"])\n",
    "val_dataset   = promptCompleDataset(tokenized_datasets[\"validation\"])\n",
    "test_dataset  = promptCompleDataset(tokenized_datasets[\"test\"])\n",
    "\n",
    "# DataLoader parameters (modifiable)\n",
    "batch_size = 10  # training and evaluation batch size (can adjust based on hardware)\n",
    "\n",
    "# Optionally use a subset of the training data for quicker iteration (for experimentation)\n",
    "train_subset_size = 1000   # use only 1000 training examples for faster training; set to len(train_dataset) to use full data\n",
    "all_indices = list(range(len(train_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "subset_indices = all_indices[:train_subset_size]\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(Subset(train_dataset, subset_indices), batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc1c8e",
   "metadata": {},
   "source": [
    "## Compressed Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3136d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1044f7a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rc_experiment.rc_experiment.data_loading import torch_data_loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import random\n",
    "\n",
    "# Obtian the DataLoader dictionary\n",
    "loader_dict = torch_data_loader(tokenized_datasets, batch_size=2)\n",
    "# Unpack the loader\n",
    "train_loader = loader_dict[\"train_loader\"]\n",
    "val_loader = loader_dict[\"val_loader\"]\n",
    "test_loader = loader_dict[\"test_loader\"]\n",
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b6201",
   "metadata": {},
   "source": [
    "# Load in Quantized Model & Register LoRA method to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45033b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U transformers accelerate bitsandbytes peft  # Install necessary packages (if not already installed)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration for fine-tuning (you can adjust these parameters)\n",
    "gradient_checkpointing = False  # Set True to enable gradient checkpointing for memory savings\n",
    "lora_r = 6           # LoRA rank (dimension of the low-rank matrices)\n",
    "lora_alpha = 16      # LoRA scaling factor\n",
    "lora_dropout = 0.05  # LoRA dropout\n",
    "bias=\"none\",\n",
    "task_type=\"CAUSAL_LM\"\n",
    "\n",
    "# Attempt to load the model in 8-bit mode\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        # load_in_8bit=True,\n",
    "        # device_map=\"cuda\"\n",
    "        # torch_dtype=torch.float16  # use fp16 if MPS supports it\n",
    "        # If you want to use 4-bit (QLoRA) instead, you could do:\n",
    "        # quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16),\n",
    "        # and pass quantization_config=quantization_config (while setting load_in_8bit=False).\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise SystemExit(\"❌ 8-bit quantized loading failed. Make sure you have a CUDA-compatible GPU and `bitsandbytes` installed. Aborting.\") \n",
    "\n",
    "# Prepare model for k-bit (here 8-bit) training – e.g., cast layer norms to float32 for stability\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Enable gradient checkpointing if configured (saves memory at cost of compute speed)\n",
    "if gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# Configure LoRA and wrap the model with LoRA adapters on all linear layers\n",
    "# (These hyperparameters can be changed to fine-tune LoRA behavior)\n",
    "target_modules = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        target_modules.append(name.split('.')[-1])\n",
    "target_modules = list(set(target_modules))\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"  # for causal language modeling\n",
    ")\n",
    "# Apply LoRA to the base model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.print_trainable_parameters()  # Display the number of trainable parameters (LoRA) vs total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d2779f",
   "metadata": {},
   "source": [
    "## Compressed Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rc_experiment.rc_experiment.model_loading import quanti_lora_md\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# The dictionary\n",
    "lora_config_kwargs = {\n",
    "    \"r\": 6,               # LoRA rank\n",
    "    \"lora_alpha\": 16,       # LoRA scaling factor\n",
    "    \"lora_dropout\": 0.05,   # LoRA dropout\n",
    "    \"bias\": \"none\",         # Bias handling\n",
    "    \"task_type\": \"CAUSAL_LM\" # Task type\n",
    "}\n",
    "\n",
    "# load the quantized lora model\n",
    "model = quanti_lora_md(lora_config_kwargs, model_name)\n",
    "# move the model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08775f8a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7c477",
   "metadata": {},
   "source": [
    "## Helper Funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c772ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to for decoder only model (requires left-padding for generation)\n",
    "def build_prompt_batch(input_ids, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    • Keeps existing left pads\n",
    "    • Removes everything to the right of the prompt\n",
    "    • Re-pads (on the left) so the batch is rectangular again\n",
    "    Returns a dict ready for model.generate().\n",
    "    \"\"\"\n",
    "    prompt_only = []\n",
    "    for seq, lab in zip(input_ids, labels):\n",
    "        first_comp = (lab != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "        prompt_only.append(seq[:first_comp])            # ← no right pads!\n",
    "\n",
    "    return tokenizer.pad(\n",
    "        { \"input_ids\": prompt_only },\n",
    "        padding=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce2f6a",
   "metadata": {},
   "source": [
    "## Training Loop with AdamW Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define optimizer (AdamW) to update only trainable params (LoRA adapters)\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "\n",
    "num_epochs = 1  # you can adjust the number of fine-tuning epochs\n",
    "patience = 2    # early stopping patience\n",
    "min_delta = 0.0 # minimum change in val loss to qualify as an improvement\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []  # track exact-match accuracy on validation\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss_total = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Compute validation loss\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss_total += outputs.loss.item()\n",
    "\n",
    "            # Load in prompt for later prediction \n",
    "            prompt_batch = build_prompt_batch(input_ids, labels)\n",
    "            prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "            preds = model.generate(\n",
    "                **prompt_batch,\n",
    "                max_new_tokens=max_target_length,\n",
    "                pad_token_id=tokenizer.pad_token_id,   # good practice\n",
    "            )\n",
    "            \n",
    "            # Compare predictions with true completions for exact match accuracy\n",
    "            for i, pred_ids in enumerate(preds):\n",
    "                pred_ids = pred_ids.tolist()\n",
    "                # Remove the prompt part from the generated sequence\n",
    "                prompt_len = (labels[i] != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "                generated_tokens = pred_ids[prompt_len:]\n",
    "                pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "                # Decode the true completion (labels where label != -100)\n",
    "                true_ids = labels[i][labels[i] != -100]\n",
    "                true_text = tokenizer.decode(true_ids.tolist(), skip_special_tokens=True)\n",
    "\n",
    "                # .contain\n",
    "                if true_text.strip() in pred_text.strip():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    avg_val_loss = val_loss_total / len(val_loader)\n",
    "    val_em = correct / total if total > 0 else 0.0  # exact match accuracy\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_em)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val EM: {val_em*100:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(f\"./best_model/{model_name}\")  # Save immediately\n",
    "        tokenizer.save_pretrained(f\"./best_model/{model_name}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3255bd0",
   "metadata": {},
   "source": [
    "## Compressed Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e859b6c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define optimizer (AdamW) to update only trainable params (LoRA adapters)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-5\u001b[39m\n\u001b[0;32m----> 7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Training config\u001b[39;00m\n\u001b[1;32m     10\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# you can adjust the number of fine-tuning epochs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/torch/optim/adamw.py:52\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     41\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     42\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/torch/optim/optimizer.py:273\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    271\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "from rc_experiment.rc_experiment.training import casual_llm_train\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define optimizer (AdamW) to update only trainable params (LoRA adapters)\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=learning_rate)\n",
    "\n",
    "# Training config\n",
    "num_epochs = 10  # you can adjust the number of fine-tuning epochs\n",
    "patience = 3    # early stopping patience\n",
    "min_delta = 0.0 # minimum change in val loss to qualify as an improvement\n",
    "\n",
    "\n",
    "best_model_dir = casual_llm_train(model_name, model, tokenizer, optimizer, train_loader, val_loader, device,\n",
    "                                  max_target_length, num_epochs, patience, min_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba8760",
   "metadata": {},
   "source": [
    "# Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913f9647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/30 [00:00<?, ?batch/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "Evaluating: 100%|██████████| 30/30 [00:16<00:00,  1.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Exact Match Accuracy: 99.67% (299/300 correctly matched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Load in prompt for later prediction \n",
    "        prompt_batch = build_prompt_batch(input_ids, labels, tokenizer)\n",
    "        prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "        preds = model.generate(\n",
    "            **prompt_batch,\n",
    "            max_new_tokens=max_target_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,  # good practice\n",
    "        )\n",
    "            \n",
    "        # Compare each generated completion with the true completion\n",
    "        for i, pred_ids in enumerate(preds):\n",
    "            pred_ids = pred_ids.tolist()\n",
    "            prompt_len = (labels[i] != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "            generated_tokens = pred_ids[prompt_len:]\n",
    "            pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            true_ids = labels[i][labels[i] != -100]\n",
    "            true_text = tokenizer.decode(true_ids.tolist(), skip_special_tokens=True)\n",
    "\n",
    "            # .contain\n",
    "            if true_text.strip() in pred_text.strip():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "test_em_accuracy = (correct / total) if total > 0 else 0.0\n",
    "print(f\"Test Exact Match Accuracy: {test_em_accuracy*100:.2f}% ({correct}/{total} correctly matched)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rc_experiment.rc_experiment.eval import rc_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a6a9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from rc_experiment.rc_experiment.training import _build_prompt_batch\n",
    "\n",
    "\n",
    "# Helper function to mannually pad prompt for decoder only model (requires left-padding for generation)\n",
    "def _build_prompt_batch(input_ids, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    • Keeps existing left pads\n",
    "    • Removes everything to the right of the prompt\n",
    "    • Re-pads (on the left) so the batch is rectangular again\n",
    "    Returns a dict ready for model.generate().\n",
    "    \"\"\"\n",
    "    prompt_only = []\n",
    "    for seq, lab in zip(input_ids, labels):\n",
    "        first_comp = (lab != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "        prompt_only.append(seq[:first_comp])            # ← no right pads!\n",
    "\n",
    "    return tokenizer.pad(\n",
    "        {\"input_ids\": prompt_only},\n",
    "        padding=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding_side=\"left\"   # <--- explicitly force left padding\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def rc_eval(test_loader, model_obj, tokenizer_obj, device, max_input_length, max_target_length):\n",
    "\n",
    "    tokenizer_obj.padding_side = 'left'\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    rows = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Load in prompt for later prediction \n",
    "            prompt_batch = _build_prompt_batch(input_ids, labels, tokenizer)\n",
    "            prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "            # Generate preidciont based on prompt\n",
    "            preds = model.generate(\n",
    "                **prompt_batch,\n",
    "                max_new_tokens=max_target_length,\n",
    "                pad_token_id=tokenizer.pad_token_id,  # good practice\n",
    "            )\n",
    "                        \n",
    "            # Store the test set prediction to a data frame\n",
    "            for i, pred_ids in enumerate(tqdm(preds, desc=\"Decoding predictions\", leave=False, unit=\"sample\")):\n",
    "                # Compare predictions with true completions\n",
    "                # Prediction text\n",
    "                pred_text = tokenizer_obj.decode(pred_ids, skip_special_tokens=True)\n",
    "                \n",
    "                # True text\n",
    "                true_ids  = labels[i]\n",
    "                true_ids  = true_ids[true_ids != -100]      # strip ignore index\n",
    "                true_text = tokenizer_obj.decode(true_ids, skip_special_tokens=True)\n",
    "                \n",
    "                # Count the number of correct and total prediction\n",
    "                if pred_text.strip() == true_text.strip():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "                # For every item in the batch, collect prompt / pred / truth\n",
    "                prompt_text = tokenizer_obj.decode(input_ids[i], skip_special_tokens=True)\n",
    "                rows.append({\n",
    "                    \"Prompt\": prompt_text.strip(),\n",
    "                    \"Prediction\": pred_text.strip(),\n",
    "                    \"Ground‑Truth\": true_text.strip(),\n",
    "                    \"Exact Match\": \"✅\" if pred_text.strip() == true_text.strip() else \"❌\"\n",
    "                })\n",
    "    test_em_accuracy = correct / total if total else 0.0\n",
    "    print(f\"Test Exact Match Accuracy: {test_em_accuracy*100:.2f}% \"\n",
    "        f\"({correct}/{total} correctly matched)\")\n",
    "    \n",
    "    # Return the prediction data frame\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "N_DISPLAY = 100  # number of examples to display\n",
    "model.eval()\n",
    "display_rows = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Wrap test_loader with tqdm\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting on test set\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Load in prompt for later prediction \n",
    "        prompt_batch = build_prompt_batch(input_ids, labels)\n",
    "        prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "        preds = model.generate(\n",
    "            **prompt_batch,\n",
    "            max_new_tokens=max_target_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,  # good practice\n",
    "        )\n",
    "        \n",
    "        # Store the test set prediction to a data frame\n",
    "        for i, pred_ids in enumerate(tqdm(preds, desc=\"Decoding predictions\", leave=False, unit=\"sample\")):\n",
    "            if len(display_rows) >= N_DISPLAY:\n",
    "                break\n",
    "            prompt_len = (labels[i] != -100).nonzero(as_tuple=True)[0][0].item()  # fix prompt_len inside the loop\n",
    "            prompt_text = tokenizer.decode(input_ids[i][:prompt_len].tolist(), skip_special_tokens=True)\n",
    "            pred_tokens = pred_ids.tolist()[prompt_len:]\n",
    "            pred_text = tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "            true_text = tokenizer.decode(labels[i][labels[i] != -100].tolist(), skip_special_tokens=True)\n",
    "            display_rows.append((prompt_text, pred_text, true_text))\n",
    "        if len(display_rows) >= N_DISPLAY:\n",
    "            break\n",
    "\n",
    "# Display the collected examples\n",
    "df = pd.DataFrame(display_rows, columns=[\"Prompt\", \"Generated Completion\", \"True Completion\"])\n",
    "display(df.head(N_DISPLAY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_result(id):\n",
    "    print(df[\"Prompt\"][id])\n",
    "    print(df[\"Generated Completion\"][id])\n",
    "    print(df[\"True Completion\"][id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "see_result(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350dd3c",
   "metadata": {},
   "source": [
    "## Compressed Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649799e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/150 [00:00<?, ?batch/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n",
      "Evaluating: 100%|██████████| 150/150 [00:42<00:00,  3.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Exact Match Accuracy: 0.00% (0/300 correctly matched)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = {\"test\":\"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/d2p_prompts_test.jsonl\"}\n",
    "from rc_experiment.rc_experiment.data_loading import raw_2_llm_data, torch_data_loader\n",
    "from rc_experiment.rc_experiment.eval import rc_eval\n",
    "\n",
    "test_datasets, tokenizer, device = raw_2_llm_data(test_path, model_name, max_input_length, max_target_length)\n",
    "# Obtian the DataLoader dictionary\n",
    "test_loader_dict = torch_data_loader(test_datasets, batch_size=2)\n",
    "test_loader = test_loader_dict[\"test_loader\"]\n",
    "\n",
    "pred_rslt_df = rc_eval(test_loader, model, tokenizer, device, max_input_length, max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8531b7",
   "metadata": {},
   "source": [
    "# Testing on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ef90fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to mannually pad prompt for decoder only model (requires left-padding for generation)\n",
    "def build_prompt_batch(input_ids, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    • Keeps existing left pads\n",
    "    • Removes everything to the right of the prompt\n",
    "    • Re-pads (on the left) so the batch is rectangular again\n",
    "    Returns a dict ready for model.generate().\n",
    "    \"\"\"\n",
    "    prompt_only = []\n",
    "    for seq, lab in zip(input_ids, labels):\n",
    "        first_comp = (lab != -100).nonzero(as_tuple=True)[0][0].item()\n",
    "        prompt_only.append(seq[:first_comp])            # ← no right pads!\n",
    "\n",
    "    return tokenizer.pad(\n",
    "        { \"input_ids\": prompt_only },\n",
    "        padding=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbfab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader Helper\n",
    "\n",
    "def build_test_loader(jsonl_path: str,\n",
    "                      batch_size: int = 8,\n",
    "                      num_examples: int | None = None):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader for an unseen JSONL test file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    jsonl_path   : str\n",
    "        Path to the JSONL file, each line: {\"prompt\": \"...\", \"completion\": \"...\"}.\n",
    "    batch_size   : int\n",
    "        Batch size for evaluation.\n",
    "    num_examples : int or None\n",
    "        If set, truncate the dataset to the first N rows (quick debug).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.utils.data.DataLoader\n",
    "        Ready-to-use test loader (no shuffling).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1️⃣  Load raw JSONL via HF datasets\n",
    "    raw_test = load_dataset(\"json\", data_files={\"test\": jsonl_path})[\"test\"]\n",
    "    if num_examples:\n",
    "        raw_test = raw_test.select(range(num_examples))\n",
    "\n",
    "    # 2️⃣  Tokenize with the same settings used during training\n",
    "    def _preprocess(ex):\n",
    "        inp = tokenizer(ex[\"prompt\"],\n",
    "                        max_length=max_input_length,\n",
    "                        truncation=True, padding=\"max_length\")\n",
    "        out = tokenizer(ex[\"completion\"],\n",
    "                        max_length=max_target_length,\n",
    "                        truncation=True, padding=\"max_length\")\n",
    "        # mask pad tokens in labels with -100\n",
    "        ex[\"input_ids\"]      = inp[\"input_ids\"]\n",
    "        ex[\"attention_mask\"] = inp[\"attention_mask\"]\n",
    "        ex[\"labels\"] = [\n",
    "            t if t != tokenizer.pad_token_id else -100\n",
    "            for t in out[\"input_ids\"]\n",
    "        ]\n",
    "        return ex\n",
    "\n",
    "    tok_test = raw_test.map(_preprocess, remove_columns=raw_test.column_names)\n",
    "\n",
    "    # 3️⃣  Torch Dataset wrapper (simple tensors)\n",
    "    class TDataset(Dataset):\n",
    "        def __init__(self, hf_ds):\n",
    "            data = hf_ds[:]\n",
    "            self.input_ids      = torch.tensor(data[\"input_ids\"])\n",
    "            self.attention_mask = torch.tensor(data[\"attention_mask\"])\n",
    "            self.labels         = torch.tensor(data[\"labels\"])\n",
    "        def __len__(self): return len(self.input_ids)\n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                \"input_ids\"     : self.input_ids[idx],\n",
    "                \"attention_mask\": self.attention_mask[idx],\n",
    "                \"labels\"        : self.labels[idx],\n",
    "            }\n",
    "\n",
    "    test_ds = TDataset(tok_test)\n",
    "\n",
    "    # 4️⃣  DataLoader (no shuffle)\n",
    "    return DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb438091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61e030f06d249eba99a893a8db9084e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader = build_test_loader(\"/Users/yifanyu/Desktop/LLM finetuning pipeline/pipeline_task/pipeline_test_data/p2d_prompts_test.jsonl\")\n",
    "\n",
    "max_target_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7448c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0e8e5ee4eb4bfbb490a6a24bfd9087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/38 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m labels         \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Generate predictions for each prompt in the batch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_target_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# silences pad-token warning\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# For every item in the batch, collect prompt / pred / truth\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, pred_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(preds):\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/peft/peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/generation/utils.py:3476\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3475\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3476\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "\n",
    "# How many examples to display\n",
    "N_DISPLAY = 20\n",
    "\n",
    "model.eval()\n",
    "rows = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Generate predictions for each prompt in the batch\n",
    "        preds = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_target_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,  # silences pad-token warning\n",
    "        )\n",
    "\n",
    "        # For every item in the batch, collect prompt / pred / truth\n",
    "        for j, pred_ids in enumerate(preds):\n",
    "            prompt_text = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
    "            pred_text   = tokenizer.decode(pred_ids,     skip_special_tokens=True)\n",
    "\n",
    "            true_ids = labels[j][labels[j] != -100]\n",
    "            true_text = tokenizer.decode(true_ids, skip_special_tokens=True)\n",
    "\n",
    "            rows.append({\n",
    "                \"Prompt\": prompt_text.strip(),\n",
    "                \"Prediction\": pred_text.strip(),\n",
    "                \"Ground‑Truth\": true_text.strip(),\n",
    "                \"Exact Match\": \"✅\" if pred_text.strip() == true_text.strip() else \"❌\"\n",
    "            })\n",
    "            if len(rows) >= N_DISPLAY:\n",
    "                break\n",
    "        if len(rows) >= N_DISPLAY:\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfebf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93322b387c5c4baf97102676012f45b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/38 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[36], line 17\u001b[0m\n",
      "\u001b[1;32m     14\u001b[0m prompt_batch \u001b[38;5;241m=\u001b[39m build_prompt_batch(input_ids, labels, tokenizer)\n",
      "\u001b[1;32m     15\u001b[0m prompt_batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m prompt_batch\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;32m---> 17\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt_batch\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_target_length\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# good practice\u001b[39;49;00m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compare predictions with true completions\u001b[39;00m\n",
      "\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pred_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(preds):\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/peft/peft_model.py:1875\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1873\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m   1874\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n",
      "\u001b[0;32m-> 1875\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   1877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/hf/lib/python3.10/site-packages/transformers/generation/utils.py:2256\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   2248\u001b[0m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n",
      "\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "\u001b[1;32m   2250\u001b[0m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n",
      "\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "\u001b[1;32m   2252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[1;32m   2253\u001b[0m         generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   2254\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m batch_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m   2255\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs_tensor\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;32m-> 2256\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43minputs_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;32m   2257\u001b[0m     ):\n",
      "\u001b[1;32m   2258\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n",
      "\u001b[1;32m   2259\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   2260\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   2261\u001b[0m         )\n",
      "\u001b[1;32m   2263\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n",
      "\u001b[1;32m   2264\u001b[0m \u001b[38;5;66;03m# decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\u001b[39;00m\n",
      "\u001b[1;32m   2265\u001b[0m \u001b[38;5;66;03m# generating the first new token or not, and we only want to use the embeddings for the first new token)\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm   # auto picks notebook-friendly bar if available\n",
    "\n",
    "model.eval()                # ensure model is in evaluation mode\n",
    "correct = 0\n",
    "total   = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Load in prompt for later prediction \n",
    "        prompt_batch = buildx_prompt_batch(input_ids, labels, tokenizer)\n",
    "        prompt_batch = {k: v.to(device) for k, v in prompt_batch.items()}\n",
    "\n",
    "        preds = model.generate(\n",
    "            **prompt_batch,\n",
    "            max_new_tokens=max_target_length,\n",
    "            pad_token_id=tokenizer.pad_token_id,  # good practice\n",
    "        )\n",
    "\n",
    "        # Compare predictions with true completions\n",
    "        for i, pred_ids in enumerate(preds):\n",
    "            pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "            true_ids  = labels[i]\n",
    "            true_ids  = true_ids[true_ids != -100]      # strip ignore index\n",
    "            true_text = tokenizer.decode(true_ids, skip_special_tokens=True)\n",
    "\n",
    "            if pred_text.strip() == true_text.strip():\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "test_em_accuracy = correct / total if total else 0.0\n",
    "print(f\"Test Exact Match Accuracy: {test_em_accuracy*100:.2f}% \"\n",
    "      f\"({correct}/{total} correctly matched)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e4297",
   "metadata": {},
   "source": [
    "## Compressed Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74741d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5241ebc",
   "metadata": {},
   "source": [
    "# Model Loading Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ec0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b6848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/yifanyu/miniconda3/envs/hf/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=6, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=6, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): lora.Linear(\n",
       "        (base_layer): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=2048, out_features=6, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=6, out_features=128256, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "        (lora_magnitude_vector): ModuleDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"   # or whatever your base model was\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "lora_weights_path = \"/Users/yifanyu/Desktop/LLM finetuning pipeline/best_model/meta-llama/Llama-3.2-1B\"  # the folder where you saved the LoRA\n",
    "model = PeftModel.from_pretrained(base_model, lora_weights_path)\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
